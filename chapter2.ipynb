{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark dataframe notes\n",
    "\n",
    "- What are PySpark Dataframes\n",
    "- Reading the Dataset\n",
    "- Checking the datatypes and coloumn (schema)\n",
    "- Other similar operations\n",
    "- Basically we are going to see how to do some data pre-processing with pyspark\n",
    "\n",
    "### Questions\n",
    "\n",
    "??? look up the actual functions in the documentation, do you have to give an `appName` what does `getOrCreate()` actually do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyspark dataframes are **lazily evaluated** this means that coputation is delayed until the value is needed, this is memeory efficent as uneccessary operations are avoided, allows working with theoretically infinte data since only the reuqired protion sn computed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "csv_path = './titanic.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.101:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10981fd60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how you actually start a pyspark session \n",
    "spark=SparkSession.builder.appName('DataFrame').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+------+----+-----------------------+-----------------------+-------+\n",
      "|Survived|Pclass|                Name|   Sex| Age|Siblings/Spouses Aboard|Parents/Children Aboard|   Fare|\n",
      "+--------+------+--------------------+------+----+-----------------------+-----------------------+-------+\n",
      "|       0|     3|Mr. Owen Harris B...|  male|22.0|                      1|                      0|   7.25|\n",
      "|       1|     1|Mrs. John Bradley...|female|38.0|                      1|                      0|71.2833|\n",
      "|       1|     3|Miss. Laina Heikk...|female|26.0|                      0|                      0|  7.925|\n",
      "|       1|     1|Mrs. Jacques Heat...|female|35.0|                      1|                      0|   53.1|\n",
      "|       0|     3|Mr. William Henry...|  male|35.0|                      0|                      0|   8.05|\n",
      "|       0|     3|     Mr. James Moran|  male|27.0|                      0|                      0| 8.4583|\n",
      "|       0|     1|Mr. Timothy J McC...|  male|54.0|                      0|                      0|51.8625|\n",
      "|       0|     3|Master. Gosta Leo...|  male| 2.0|                      3|                      1| 21.075|\n",
      "|       1|     3|Mrs. Oscar W (Eli...|female|27.0|                      0|                      2|11.1333|\n",
      "|       1|     2|Mrs. Nicholas (Ad...|female|14.0|                      1|                      0|30.0708|\n",
      "+--------+------+--------------------+------+----+-----------------------+-----------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Siblings/Spouses Aboard: integer (nullable = true)\n",
      " |-- Parents/Children Aboard: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      "\n",
      "['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dateset, note that infer schema allows for finding types\n",
    "spark_df = spark.read.option(\"header\", \"true\").csv(csv_path, inferSchema = True)\n",
    "spark_df.show(10)\n",
    "\n",
    "# Check the schema\n",
    "spark_df.printSchema()\n",
    "\n",
    "# get coulumn names \n",
    "print(spark_df.columns)\n",
    "\n",
    "# Note that if I print like this I ge the rows\n",
    "spark_df.head(2)\n",
    "\n",
    "# Selecting a single column\n",
    "type(spark_df.select('Name')) # You can also select muliple columns by passing a list\n",
    "\n",
    "# NOTE: this returns a coloumn object wich is diffrent and is not for viewin the function\n",
    "# spark_df['Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handeling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional learnt info \n",
    "\n",
    "UDF user defined function e.g. that you can get in SQL "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
